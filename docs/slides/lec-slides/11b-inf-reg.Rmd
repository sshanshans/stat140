---
title: "Inference for regression"
author: "Dr. Maria Tackett"
date: "04.03.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta199-slides.css"
    logo: img/sta199-sticker-icon.png
    lib_dir: libs
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
---

layout: true

<div class="my-footer">
<span>
<a href="http://datasciencebox.org" target="_blank">datasciencebox.org</a>
</span>
</div> 

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(DT)
library(emo)
library(openintro)
library(infer)
library(gridExtra)
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
opts_chunk$set(fig.height = 2.5, fig.width = 5, dpi = 300) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

---

## Announcements

- Midterm due Friday at 11:59p
    - Team Feedback 2 due Friday at 11:59p
  
- Extra Credit assigned next week

---

class: center, middle

## Inference for regression

---

## Riders in Florence, MA

.small[
The Pioneer Valley Planning Commission collected data in Florence, MA for 90 days from April 5 to November 15, 2005 using a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.

- `hightemp` daily high temperature (in degrees Fahrenheit)
- `volume` estimated number of trail users that day (number of breaks recorded)
]

```{r}
library(mosaicData)
data(RailTrail)
```

---

## Riders in Florence, MA

- `hightemp` daily high temperature (in degrees Fahrenheit)
- `volume` estimated number of trail users that day (number of breaks recorded)

```{r echo=FALSE, fig.height=2.25}
ggplot(data = RailTrail, mapping = aes(x = hightemp, y = volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  xlim(40, 100) +
  ylim(120, 750) +
  labs(ylab = "Volume", xlab)
```


---

## Coefficient interpretation

.question[
`r emo::ji("bust_in_silhouette")` Interpret the coefficients of the regression model for predicting volume (estimated number of trail users that day) from hightemp (daily high temperature, in F).
]

```{r}
m_riders <- lm(volume ~ hightemp, data = RailTrail)
tidy(m_riders) %>%
  select(term, estimate)
```



---

## Uncertainty around the slope

```{r echo=FALSE}
ggplot(data = RailTrail, mapping = aes(x = hightemp, y = volume)) +
  geom_point() +
  geom_smooth(method = "lm", color = "black") +
  xlim(40, 100) +
  ylim(120, 750)
```

---


### Bootstrapping the data, once

```{r echo=FALSE}
set.seed(18472)

boot_samples <- RailTrail %>%
  specify(volume ~ hightemp) %>% 
  generate(reps = 50, type = "bootstrap")

first_boot_sample <- boot_samples %>%
  filter(replicate == 1)

ggplot(first_boot_sample, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  scale_color_manual(values = "gray") +
  xlim(40, 100) +
  ylim(120, 750)
```

```{r echo=FALSE}
m_boot <- lm(volume ~ hightemp, data = first_boot_sample)
tidy(m_boot) %>%
  select(term, estimate)
```

---

### Bootstrapping the data, again

```{r echo=FALSE}
second_boot_sample <- boot_samples %>%
  filter(replicate == 2)

ggplot(second_boot_sample, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  scale_color_manual(values = "gray") +
  xlim(40, 100) +
  ylim(120, 750)
```

```{r echo=FALSE}
m_boot <- lm(volume ~ hightemp, data = second_boot_sample)
tidy(m_boot) %>%
  select(term, estimate)
```

---

### ...and again

```{r echo=FALSE}
third_boot_sample <- boot_samples %>%
  filter(replicate == 3)

ggplot(third_boot_sample, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  scale_color_manual(values = "gray") +
  xlim(40, 100) +
  ylim(120, 750)
```

```{r echo=FALSE}
m_boot <- lm(volume ~ hightemp, data = third_boot_sample)
tidy(m_boot) %>%
  select(term, estimate)
```

---

## Bootstrapping the regression line

```{r echo=FALSE}
ggplot(boot_samples, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_smooth(method = "lm", se = FALSE, lwd = 0.2) +
  geom_abline(slope = m_riders$coefficients[2], intercept = m_riders$coefficients[1], lwd = 1, color = "black") +
  theme(legend.position = "none") +
  scale_color_manual(values = rep("gray", 100)) +
  ylim(100, 750)
```

---

## Bootstrap interval for the slope

.small[
```{r}
boot_dist <- RailTrail %>%
  specify(volume ~ hightemp) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "slope")
```
]

```{r echo=FALSE, warning = F}
ci <- boot_dist %>%
  summarise(l = quantile(stat, 0.025), u = quantile(stat, 0.975))

ggplot(data = boot_dist, mapping = aes(x = stat)) +
  geom_histogram(binwidth = 0.25) +
  geom_vline(xintercept = ci$l, color = color_palette$darkblue) +
  geom_vline(xintercept = ci$u, color = color_palette$darkblue)
```

---

## Bootstrap interval for the slope

.question[
Interpret the bootstrap interval in context of the data.
]

```{r}
boot_dist %>%
  summarise(l = quantile(stat, 0.025), 
            u = quantile(stat, 0.975))
```

---

## Hypothesis testing for the slope

$H_0$: No relationship, $\beta_1 = 0$  
$H_A$: There is a relationship, $\beta_1 \ne 0$

--

.small[
```{r}
null_dist <- RailTrail %>%
  specify(response = volume, explanatory = hightemp) %>% 
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")
```
]

--

```{r echo=FALSE, fig.height = 2.25, warning = F, message = F}
ggplot(data = null_dist, mapping = aes(x = stat)) +
  geom_histogram(binwidth = 0.25) +
  xlim(-6, 6) +
  geom_vline(xintercept = m_riders$coefficients[2], color = color_palette$red) +
  geom_vline(xintercept = -1*m_riders$coefficients[2], color = color_palette$red) 
```

---

## Finding the p-value

```{r}
obs_slope <- tidy(m_riders) %>% 
  select(estimate) %>% 
  slice(2) %>% pull()

null_dist %>%
  filter(stat >= obs_slope) %>%
  summarise(p_val = 2 * (n()/1000))
```

---

## Hypothesis testing for the slope

... using the Central Limit Theorem

```{r}
tidy(m_riders)
```

---

### Conditions for inference for regression

Four conditions:

--

1. Observations should be independent

--

2. Residuals should be randomly distributed around 0

--

3. Residuals should be nearly normally distributed, centered at 0

--

4. Residuals should have constant variance


---

## Checking independence

One consideration might be time series structure of the data. We can check whether one residual depends on the previous by plotting the residuals in the order of data collection.

```{r fig.height = 2}
m_riders_aug <- augment(m_riders)
ggplot(data = m_riders_aug, aes(x = 1:nrow(m_riders_aug), y = .resid)) +
  geom_point() +
  labs(x = "Index", y = "Residual")
```

---

### Checking distribution of residuals around 0 and constant variance

```{r, fig.height = 2}
ggplot(data = m_riders_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, color = "gray") +
  labs(y = "Residuals", x = "Predicted values, y-hat")
```

---

## Checking normality of residuals

```{r}
ggplot(data = m_riders_aug, aes(x = .resid)) +
  geom_histogram(binwidth = 30) +
  labs(x = "Residuals")
```

---

### What else can we do with these p-values?

Model selection based on p-value method:

- **Backwards elimination**: Remove the variable with the highest p-value, re-fit, 
repeat until all variables are significant at the chosen significance level.
- **Forward selection**: Start with the variable with the lowest p-value, re-fit,
repeat until all no more significant variables left at the chosen significance level.

This is sometimes seen in the literature, but is not recommended!

- Relies on arbitrary significance level cutoff
- Multiple testing!

Instead use adjusted $R^2$, or AIC, or other criterion based model selection.

---

## Thoughts...

Both sets of p-values are largely useless other than in a few very narrow circumstances

* Coefficient p-value 
    - If you truly want to know if a coefficient is significantly different from zero (taking the other predictors into account) then use the p-value
    - If you want to know which predictors are important - use model selection

* Overall model p-value
    - Strawman comparison, I don't really care that my model is better than a mean only model, the latter is almost always going to be terrible

